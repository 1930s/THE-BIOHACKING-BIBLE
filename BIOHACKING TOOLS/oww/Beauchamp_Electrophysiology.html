{{Beauchamp Lab Notebook Navigation Bar}}
<br>
<div style="padding: 8px; color: #000000; background-color: #ffffff; width: 730px; border: 2px solid #666666;">

==Electrophysiology Protocols==

[[Beauchamp:Presurgical Scanning|Presurgical Scanning]]

After analysing fMRI data, upload the entire contents of the AFNI and SUMA directories to Xfiles.
This can be simplfied by Apple-K (Connect to Server) in Finder and choosing XFiles;
  xfiles.hsc.uth.tmc.edu (129.106.148.217)
then the folders can be dragged from the server to Xfiles, or copied in the command line, easily (without using the Web-based GUI interface).


<i>In the EMU</i>

[[Beauchamp:Setup Apparatus|Setup Apparatus]]

[[Beauchamp:Receptive Field Mapping|Receptive Field Mapping]]

[[Beauchamp:Electrical Stimulation|Electrical Stimulation]]

[[Beauchamp:Selectivity|Selectivity]]

[[Beauchamp:Perceptual Biasing|Perceptual Biasing]]

It is also good to collect 10 minutes of resting data (no stimulation) from as many visual electrodes as possible for later analyses.

==Todo list==
Decide on screening stimuli i.e. pick 20 from each category
faces, houses, bodies, scenes, tools, scrambles <br>
Get rid of bad looking stimuli; make detailed protocol <br>
Install matlab in EMU to allow image scrambling; install scrambling program. <br>


1/16/2008
Add peak deflection (either -,+ or ABS) to RMS power measurement when ranking stimuli.

==January 2008 Subjects==
Proposed experiments for January 2008 subjects.


Focus on ventral temporal and lateral occipital-temporal electrodes with visual responses in fMRI
not on electrodes over early visual cortex<br><br>
EXPERIMENT: object selectivity to determine preferred and nonpreferred stimuli with 5 well-defined categories, including: faces, bodies, houses, tools, scrambles.<br>
PLUGIN NAME: HumanObjectSelectivity <br>
Can rank stimuli by RMS power in a selected window, -1 is worst, -5 is 5th worst, +1 is the best, +5 is the fifth best. <br>
Channel plot has to contain interval in which you will calculate the RMS power (gray shaded region is stimulation ON time)
Only calculates preferred stimuli for one channel at a time.

METHODS: Present stimuli at their natural size, centered at the fixation point (unless electrodes are only in one hemisphere, in which case present contralateral). If possible, use large, hi-res color stimuli. If not, whatever we can get. Present stimuli for 125 ms, 375 off (2 Hz rate). Each run will be ~10 minutes, allowing ~10 reps of each stimulus. 

Task: Subjects will perform one-back repetition detection to ensure attention to the stimulus and maximize responses (SELECT IMAGE REPETITION in behavioral window). Backup task: use target image detection with very long trial duration. <br>
GOALS: Determine preferred objects of the electrode. 

1) Determine if there is a fine-grained representation of category e.g. within the FFA, are there sites that prefer stimuli from other categories?<br>
2) Is there a sharp tuning within category e.g. within FFA, does the electrode respond to only a single face or many faces?

3) Relationship to retinotopy. Malach predicts that FFA is primarily foveal, PPA primarily peripheral.


ANTICIPATED RESULT:Like in the Malach paper, there will be a sharp tuning with some electrodes only responding to stimuli in their preferred category.<br><br>
EXPERIMENT: For electrode(s) with nice clean responses to a preferred stimulus, do RF mapping with the 3 most-preferred stimuli (or fewer if necessary)<br>
PLUGIN NAME: HumanLetterDetection. <br> Pick the preferred image and then do RF mapping; have to do separate runs for different stimuli. Make Letter Target times longer (change all three).<br>
METHODS: Subjects will perform central letter detection task. <br>
GOAL: Determine RFs in higher areas (identified with fMRI)<br>
PREDICTION: Higher areas will have large but not completely homogenous spatial RFs<br>
Possibility:also map RF with less-preferred stimuli<br><br>

EXPERIMENT: stimulation at 2 (up to 8) mA (no psychometrics) to see which, if any, late sites evoke percepts<br>
If there is no percept, at the highest current do 20 trials with behavioral responses to quantify the lack of response. <br>
If there is a percept, see if it is complex or not.
If a simple phosphene in an early site, do 20 trials with behavioral response at a current to prove there was a percept.
If a complex percept or a later site, do the complete psychometric function. <br>
PLUGIN NAME: Microstim Staircase <br>
GOAL: additional data for Dona's current paper; pilot data for grant to show that stimulation in higher areas does NOT produce a percept.<br>
ANTICIPATED RESULT: few, if any, sites will produce percepts<br>
<br><br>

EXPERIMENT: stimulation of higher electrodes at 2 (or higher) mA while subject makes object or noise discrimination<br>
i.e. perceptual biasing with preferred stimuli embedded in noise<br>
PLUGIN NAME: HumanBiasPerception. <br> 3 categories of images (noise only, low noise, high noise) place in different folders, load images (images # is TOTAL # of images). <br>
Stimulus Settings are just within trial. Pre-interval is after trial start but before stimulus comes on; interval is stimulation on-screen or electrical. Inter-interval is between intervals (does not apply because only one choice). Post-choice is time between stimulation and when the response window opens. Behavior/Intertrial is a separate window: response time is length of time for response.

Detailed protocol:

STEP 1:
take preferred stimulus, add a lot of noise so the subject can only detect it 75% of the time in a 2-AFC
(e.g. FACE or NOISE?)
If there is no preferred stimulus, but strong fMRI response, use a stimulus from preferred fMRI category. 

STEP 2:
Deliver electrical stimulation while showing the pictures and see if it changes the detection rate.
PREDICTION: Will increase (or decrease the detection rate. <br>

DETAILED METHODS:
Present stimulus for 250 ms, deliver stimulation for the entire stimulation period.<br>
FEEDBACK: On no stim trials, subject receives correct, incorrect, no response feedback. On stim trials, subject receives correct (if any response is given) or no response feedback. All stim trials are correct because we do not know the subject's percept.


2x2 experimental design:
STIM or NO STIM and
PREF STIM+NOISE or NOISE

Question: for each cell, we will need multiple JPGs--is this possible?

Related experiments: 
3x3 experimental design:
HIGH CURRENT STIM, LOW CURRENT STIM, or NO STIM and
PREF STIM, PREF STIM+NOISE, NOISE

Repeat with non-preferred stimulus; see if it produces a behavioral effect.<br>

Future experiments:
Create a 2-IFC design, where each trial contains face and face+noise, subjects selects the interval containing the  face.
Stimulation is delivered in one of the epochs or neither. <br>



EXPERIMENT:10-min Resting state data (perhaps awake and asleep) <br><br>
PLUGIN NAME: Human Letter Detection, Stimulation=Eccentricity, Behavior Settings, click one trial only box, set eccentricity trial to be very long, make blocks be very big<br>

If there is ample time:<br>
EXPERIMENT: repeated presentation of preferred stimulus; repeated presentation of nonpreferred stimulus (context: letter detection foveally)<br>
PREDICTION: AAAB more than BBBB<br>
PLUGIN NAME: HumanAdaptation <br>
Tries: random sequence parameter, leave at 100

EXPERIMENT:study motion, orientation selectivity using Ping's new screening program <br>



object selectivity with preferred stimulus in big screen of same category stimuli <br>
object selectivity with preferred stimulus in big screen of nonpreferred category stimuli <br>
object selectivity with nonpreferred stimulus in big screen of same category stimuli <br>
object selectivity with nonpreferred stimulus in big screen of preferred category stimuli <br>


</div>

==Processing Subject Data==
After obtaining the CD containing the patient CT data from St. Luke's, use OsiriX to export all images
(using the export to DICOM option, and the hierarchical, uncompress options).

CT scans have voxel size 0.488x0.488x1 mm; this may need to be adjusted manually with
  3drefit -zdel 1.000 DE_CTSDE+orig
(If the CTs look distorted in AFNI, then the voxel size must be adjusted).
Next, the CTs must be registered with the hi-res presurgical MRI anatomy.
This may fail because the CT has a coordinate system with a very different origin than the MRI.
Registration routines will not work if the input datasets are not in rough alignment.
To check this, type 
  3dinfo DE_CTSDE+orig
returns
  R-to-L extent:  -124.756 [R] -to-   124.756 [L] -step-     0.488 mm [512 voxels]
  A-to-P extent:  -124.756 [A] -to-   124.756 [P] -step-     0.488 mm [512 voxels]
  I-to-S extent:  -258.000 [I] -to-   -86.000 [I] -step-     1.000 mm [173 voxels]

We want the center of the dataset to be roughly at (0,0,0). For this example, this is true for (x,y) but not for z. 
First, create a copy of the dataset
  3dcopy DE_CTSDE+orig DE_CTSDEshift
Then, recenter the z-axis 
  3drefit -zorigin 80 DE_CTSDEshift+orig
3dinfo returns
  R-to-L extent:  -124.756 [R] -to-   124.756 [L] -step-     0.488 mm [512 voxels]
  A-to-P extent:  -124.756 [A] -to-   124.756 [P] -step-     0.488 mm [512 voxels]
  I-to-S extent:   -80.000 [I] -to-    92.000 [S] -step-     1.000 mm [173 voxels]

The z-axis is now roughly centered around 0. In AFNI, examine the MR and the shifted CT to make sure they are in rough alignment. Next, use 3dAllineate to align the two datasets.
  3dAllineate -base {$ec}anatavg+orig -source DE_CTSDEshift+orig -prefix {$ec}CTSDE_REGtoanatV4 -verb -warp shift_rotate -cost mutualinfo -1Dfile {$ec}CTSDE_REGtoanatXformV4

Check in AFNI to make sure that they alignment is correct. NB: It is also possible to crop the MRI before Allineating since the MR coverage is typically greater than the CT coverage. In a test case, this did not have a big effect.

==Things to do==
HumanImageDetection
:Can stimuli be vector-based rather than pixel based, so as not to lose resolution with scaling? POSSIBLE if original file is vector-based
:Enable online scrambling LOOKING INTO IT
:Enable online color to black and white conversion LOOKING INTO IT 

HumanLetterDetection
:Analyze data from LR to see where the RFs are